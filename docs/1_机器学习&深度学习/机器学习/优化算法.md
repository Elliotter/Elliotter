# 优化算法

## 损失函数

### 分类问题
交叉熵损失函数

### 回归问题
1. 平方损失函数：平方损失函数是光滑函数，能够用梯度下降法进行优化。然而，当预测值距离真实值越远时，平方损失函数的惩罚力度越大，因此它对异常点比较敏感。

2. 绝对损失函数相当于是在做中值回归，相比做均值回归的平方损失函数，绝对损
失函数对异常点更鲁棒一些。但是，绝对损失函数在f=y处无法求导数。

3. Huber损失函数
Huber损失函数在|f−y|较小时为平方损失，在|f−y|较大时为线性损失，处处可导，
且对异常点鲁棒。

## 梯度下降算法
### 经典的梯度下降法
采用所有训练数据的平均损失来近似目标函数，大量数据时候计算成本太高

### 随机梯度下降法（SGD-mini-batch gradient descent）
在实际应用中我们会同时处理若干训练数据，该方法被称为小批量梯度下降法(Mini-Batch Gradient Descent)

**存在的缺点 ： 山谷震荡和鞍点停滞的问题**

### Momentum


### AdaGrad
Adagrad其实是对学习率进行了一个约束。前期较小的时候， regularizer较大，能够放大梯度，后期较大的时候，regularizer较小，能够约束梯度，适合处理稀疏梯度
### Adam

经过实验可以发现，优化速度来看：Sgd>Sgdm>Adagrad>Rmsprop>2.78
Sgd与Sgdm比较容易陷入局部较优解，当学习率比较小的时候，容易出不来,sgdm相比sgd多了一个Momentum，训练速度变慢了
Adagrad在学习率比较小的时候容易陷入局部最优解
Rmsprop在学习率比较大的时候容易不稳定
Adam在学习率0.1和0.01时表现都很稳定，都迅速达到了最佳解，但是时间也是最长的




